{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "761cb4ef-b9a4-41a7-a22d-d890b39e76c3",
   "metadata": {},
   "source": [
    "Поработаем над датасетом из соревнования [UtkMl](https://www.kaggle.com/competitions/utkmls-twitter-spam-detection-competition/overview) про определение спама в твитах.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a896f68d-babe-41e8-bbc7-2db43e5d625b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34741a82-f113-4c4e-a2a0-e1916a6a66b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv')\n",
    "data.drop('Unnamed: 7', axis=1, inplace=True)  \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1d3b50-153b-4c68-a69f-c3ad2cc5ee41",
   "metadata": {},
   "source": [
    "Почистим датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86ad3a8-abb6-4743-a8aa-73f873d8b672",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5737d15-509f-40d7-823d-45ffacdda788",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Type.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99d550c-0bc1-471b-a08d-4d35e1b4002e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data.Type != \"South Dakota\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab2eee7-7243-4faf-9f0a-8ac4b1667ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Type'] = data['Type'].apply(lambda x: 0 if x == 'Quality' else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebd0b14-775a-4cfc-bbd2-5c1154cbf1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop('location', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196c0d8d-dbda-45e3-850f-c9cbd78ad09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['Type']\n",
    "X = data.drop('Type', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727447a1-2710-4580-a92f-9ac04c42f65a",
   "metadata": {},
   "source": [
    "Давайте построим графики. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d5b26e-b563-4b2e-bb7c-c15f99c89ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=2)\n",
    "for ax, feat in zip(axes.flat, X.select_dtypes(include=np.number).columns):\n",
    "    sns.kdeplot(X[feat], ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5be9c77-b741-45ab-9e17-a7561fbaffe5",
   "metadata": {},
   "source": [
    "Видим, что а) ни один из признаков не имеет нормального распределения б) для небинарных признаков самое частое значение - 0. Скорее всего, наиболее важную информацию несут сами твиты (мы намеренно не будем удалять ссылки и хештеги, потому что они, скорее всего, вообще сделают самый важный вклад)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc90414-bd86-4011-bc92-a40c4aa3dad4",
   "metadata": {},
   "source": [
    "Теперь нам необходимо обработать наши признаки. Часть из них - текстовые, и их нужно обработать отдельно. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c7c1bf-baaf-4b68-b0a0-f65c114d16d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(choice_transformer, choice_ngrams):\n",
    "    # числовые характеристики нормализуем: imputer обрабатывает наны, scaler масштабирует\n",
    "    numeric_features = ['following', 'followers', 'actions', 'is_retweet']\n",
    "    numeric_transformer = Pipeline(\n",
    "        steps=[(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())]\n",
    "    )\n",
    "\n",
    "    # текстовые характеристики обрабатываем: либо tf-idf, либо мешок слов\n",
    "    text_features = 'Tweet'\n",
    "    if choice_transformer == 'tfidf':\n",
    "        text_transformer = TfidfVectorizer(ngram_range=choice_ngrams, tokenizer=word_tokenize, stop_words='english')\n",
    "    else:\n",
    "        text_transformer = CountVectorizer(ngram_range=choice_ngrams, tokenizer=word_tokenize, stop_words='english')\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, numeric_features),\n",
    "            (\"txt\", text_transformer, text_features),\n",
    "        ]\n",
    "    )\n",
    "    return preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423b37cf-f615-46da-8393-9240d979870f",
   "metadata": {},
   "source": [
    "Обучим модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a077f4c-e9df-497d-b669-4f6cff558f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv')\n",
    "test.drop('Id', axis=1, inplace=True)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c173ff-55fd-4793-9296-7b04b00b3191",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelfit(model):\n",
    "    model.fit(Xtrain, ytrain)\n",
    "    \n",
    "    ypredtest = model.predict(Xtest)\n",
    "    ypredtrain = model.predict(Xtrain)\n",
    "    \n",
    "    print(accuracy_score(ytest, ypredtest), accuracy_score(ytrain, ypredtrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afc7d71-0164-4cb4-b116-90c5adcdf6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bf2a88-6878-4100-aca6-f83b57c4b460",
   "metadata": {},
   "source": [
    "TF-IDF unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068df0f5-4aad-4366-b05a-ee18669b6f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = feature_engineering('tfidf', (1, 1))\n",
    "\n",
    "clfLR = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"classifier\", LogisticRegression())]\n",
    ")\n",
    "\n",
    "clfSVC = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"classifier\", SVC())]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b81995c-6b6c-47f4-afed-5971ddd515f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelfit(clfLR)\n",
    "modelfit(clfSVC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4faad9-aa15-40c4-909a-03427bed66b7",
   "metadata": {},
   "source": [
    "TF-IDF bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ea26a0-c231-4d3f-8ec3-cc539696a2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = feature_engineering('tfidf', (2, 2))\n",
    "\n",
    "clfLR = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"classifier\", LogisticRegression())]\n",
    ")\n",
    "\n",
    "clfSVC = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"classifier\", SVC())]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe658882-94c8-4c6a-83b9-8bc4b4c83747",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelfit(clfLR)\n",
    "modelfit(clfSVC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570b527f-5a2c-42b8-9582-97bf1007d75a",
   "metadata": {},
   "source": [
    "BOW unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae12084d-c5a6-44ea-9c5f-270241fbf806",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = feature_engineering('bow', (1, 1))\n",
    "\n",
    "clfLR = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"classifier\", LogisticRegression())]\n",
    ")\n",
    "\n",
    "clfSVC = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"classifier\", SVC())]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa8b2e2-bd77-4e9a-8451-28b616bef0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelfit(clfLR)\n",
    "modelfit(clfSVC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efc25af-b0cc-498f-9196-59b2e79dd72a",
   "metadata": {},
   "source": [
    "BOW bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec833a10-2a43-4cf8-bf2b-fe349d66fa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = feature_engineering('bow', (2, 2))\n",
    "\n",
    "clfLR = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"classifier\", LogisticRegression())]\n",
    ")\n",
    "\n",
    "clfSVC = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"classifier\", SVC())]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7006367c-376f-407a-bbf2-00e9f94378c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelfit(clfLR)\n",
    "modelfit(clfSVC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826e0ab7-9193-4441-afb5-b3ece33cc7ac",
   "metadata": {},
   "source": [
    "BOW Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e033784-38c3-4e9b-b559-49d512fbf8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = feature_engineering('bow', (1, 1))\n",
    "\n",
    "bagging = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"classifier\", BaggingClassifier())]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0633012-cd4a-48cd-8e4b-f4724d702961",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelfit(bagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef959f39-9e0b-4bfa-aa85-d366aba252f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = feature_engineering('bow', (2, 2))\n",
    "\n",
    "bagging = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"classifier\", BaggingClassifier())]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffca4ee-beb6-4923-99c3-b9ea6d82b465",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelfit(bagging)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2758b30-a7ad-4098-89ee-6930be9f9092",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725da3f0-b789-4ea4-a923-7ca7f7d99c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = feature_engineering('bow', (2, 2))\n",
    "\n",
    "clf = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), (\"classifier\", RandomForestClassifier())]\n",
    ")\n",
    "\n",
    "modelfit(clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9050542b-723b-4854-bbcc-4f3233f67725",
   "metadata": {},
   "source": [
    "Проверка только на твитах (чтобы посмотреть, что влияет сильнее всего)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2eabc7-6a14-4a85-9796-be80baa1df16",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(ngram_range=(3, 3), tokenizer=word_tokenize, stop_words='english')\n",
    "bow = vec.fit_transform(Xtrain['Tweet'])\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(bow, ytrain)\n",
    "ypredtest = clf.predict(vec.transform(Xtest['Tweet']))\n",
    "print(classification_report(ypredtest, ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668fb1ba-154e-4738-b3b8-ea2c3ff1e1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(vec.vocabulary_.items())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c53652-5373-4a39-abc2-c810359f7180",
   "metadata": {},
   "source": [
    "Проверим гипотезу о том, что ссылки сильнее всего влияют на определение спама"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18298f1d-dff0-4d92-ad14-6a4d297a54f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = list(data['Tweet'])\n",
    "nums = []\n",
    "\n",
    "for num, w in enumerate(tweets):\n",
    "    if 't.co' in w:\n",
    "        nums.append(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3268106-e8dc-4b31-a544-50252afcb110",
   "metadata": {},
   "outputs": [],
   "source": [
    "spamham = list(data['Type'])\n",
    "needed = []\n",
    "for i, w in enumerate(spamham):\n",
    "    if i in nums:\n",
    "        needed.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12227072-5779-4db1-9a70-054694c2e252",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31ffa5c-4976-4cad-a2d7-a9a0136c7a82",
   "metadata": {},
   "source": [
    "Оказывается, подавляющее большинство текстов со ссылками определено как спам."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637c8f0a-31b1-4030-9250-6b46937b6368",
   "metadata": {},
   "source": [
    "А теперь давайте посмотрим, как можно было ту же задачу решить с помощью эмбеддингов Doc2Vec из gensim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe54d55-80f3-4ef7-83e1-8fb65fd32c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4394f25b-4416-4c94-9eac-e544b9cb6c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "import gensim\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from sklearn import utils\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72df8abc-58c4-4263-8970-08b8f656a5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0734942d-3f23-454d-9978-6f1d67944692",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def tokenize_text(text):\n",
    "    '''gensim сам токенизировать не умеет, поэтому нам придется сделать это за него'''\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word.lower())\n",
    "    return tokens\n",
    "\n",
    "train, test = train_test_split(data[['Tweet', 'Type']], test_size=0.3, random_state=42)\n",
    "\n",
    "# соберем специальный объект класса TaggedDocument, чтобы D2V работал\n",
    "train_tagged = train.apply(\n",
    "    lambda r: TaggedDocument(words=tokenize_text(r['Tweet']), tags=[r.Type]), axis=1)\n",
    "test_tagged = test.apply(\n",
    "    lambda r: TaggedDocument(words=tokenize_text(r['Tweet']), tags=[r.Type]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22258bfc-6d37-4170-af79-911ca044f878",
   "metadata": {},
   "source": [
    "Воспользуемся тем, что у большинства современных процессоров больше одного ядра..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb43bf50-547c-47b8-9173-085198af0371",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "cores = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27a2c9f-33ff-49d5-bab3-bbbe928ca1f3",
   "metadata": {},
   "source": [
    "Обучим модельку DBoW (Distributed Bag of Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90869a91-564d-40cc-80d6-fee5bd73a764",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, hs=0, min_count=2, sample=0, workers=cores)\n",
    "model_dbow.build_vocab([x for x in tqdm(train_tagged.values)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c071a3e0-4089-489d-abc0-88337c995c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(30):\n",
    "    model_dbow.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n",
    "    model_dbow.alpha -= 0.002\n",
    "    model_dbow.min_alpha = model_dbow.alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e9adc0-23c5-4934-86ae-990d0dff1caf",
   "metadata": {},
   "source": [
    "Напишем аналог transform для D2V:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74264ac-dc40-437d-8b03-d7954cea9822",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_for_learning(model, tagged_docs):\n",
    "    sents = tagged_docs.values\n",
    "    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words)) for doc in sents])\n",
    "    return targets, regressors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947130a0-6eb2-4595-8084-e88a58a99f38",
   "metadata": {},
   "source": [
    "Обучим банальную логистическую регрессию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa49355-ceaf-4aaf-af8e-b7c0471e2f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, X_train = vec_for_learning(model_dbow, train_tagged)\n",
    "y_test, X_test = vec_for_learning(model_dbow, test_tagged)\n",
    "logreg = LogisticRegression(solver='liblinear', n_jobs=1, C=1e5)\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "print('Testing accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('Testing F1 score: {}'.format(f1_score(y_test, y_pred, average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cdc5e0-6a8a-4ade-80fd-bffd745d260d",
   "metadata": {},
   "source": [
    "С другими алгоритмами можете побаловаться сами. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fce0db2-1f9c-49a3-bce2-39641128cbb7",
   "metadata": {},
   "source": [
    "### Задание\n",
    "\n",
    "Попробуйте поработать с датасетом юридических текстов. В датасете всего две важных колонки признаков: заголовок дела и его текст, а целевая переменная - case_outcome (мультиклассовая классификация). \n",
    "\n",
    "В базовом варианте можно оставить только текст дела, если хотите поинтереснее - можно попробовать распарсить case_title, добыв оттуда дополнительные признаки. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3185295c-e4ac-4a6e-b03d-881e0eea2b71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_id</th>\n",
       "      <th>case_outcome</th>\n",
       "      <th>case_title</th>\n",
       "      <th>case_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Case1</td>\n",
       "      <td>cited</td>\n",
       "      <td>Alpine Hardwood (Aust) Pty Ltd v Hardys Pty Lt...</td>\n",
       "      <td>Ordinarily that discretion will be exercised s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Case2</td>\n",
       "      <td>cited</td>\n",
       "      <td>Black v Lipovac [1998] FCA 699 ; (1998) 217 AL...</td>\n",
       "      <td>The general principles governing the exercise ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Case3</td>\n",
       "      <td>cited</td>\n",
       "      <td>Colgate Palmolive Co v Cussons Pty Ltd (1993) ...</td>\n",
       "      <td>Ordinarily that discretion will be exercised s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Case4</td>\n",
       "      <td>cited</td>\n",
       "      <td>Dais Studio Pty Ltd v Bullett Creative Pty Ltd...</td>\n",
       "      <td>The general principles governing the exercise ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Case5</td>\n",
       "      <td>cited</td>\n",
       "      <td>Dr Martens Australia Pty Ltd v Figgins Holding...</td>\n",
       "      <td>The preceding general principles inform the ex...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  case_id case_outcome                                         case_title  \\\n",
       "0   Case1        cited  Alpine Hardwood (Aust) Pty Ltd v Hardys Pty Lt...   \n",
       "1   Case2        cited  Black v Lipovac [1998] FCA 699 ; (1998) 217 AL...   \n",
       "2   Case3        cited  Colgate Palmolive Co v Cussons Pty Ltd (1993) ...   \n",
       "3   Case4        cited  Dais Studio Pty Ltd v Bullett Creative Pty Ltd...   \n",
       "4   Case5        cited  Dr Martens Australia Pty Ltd v Figgins Holding...   \n",
       "\n",
       "                                           case_text  \n",
       "0  Ordinarily that discretion will be exercised s...  \n",
       "1  The general principles governing the exercise ...  \n",
       "2  Ordinarily that discretion will be exercised s...  \n",
       "3  The general principles governing the exercise ...  \n",
       "4  The preceding general principles inform the ex...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('legal_text_classification.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85639436-6b32-467f-ac27-37e11f404c3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlearn",
   "language": "python",
   "name": "mlearn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
