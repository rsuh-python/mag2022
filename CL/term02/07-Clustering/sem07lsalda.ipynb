{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45f42b94-f006-4403-aa75-f6c977211279",
   "metadata": {},
   "source": [
    "### SVD в scikit-learn\n",
    "\n",
    "Поработаем над датасетом с новостями (20 классов по темам). Обработаем и векторизуем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c592c4-ac35-4e1a-87c4-2571489d9b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30451d2-29c2-4cc4-850d-ed2b70419f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmv_emails_websites(string):\n",
    "    \"\"\"Function removes emails, websites and numbers\"\"\"\n",
    "    new_str = re.sub(r\"\\S+@\\S+\", '', string)\n",
    "    new_str = re.sub(r\"\\S+.co\\S+\", '', new_str)\n",
    "    new_str = re.sub(r\"\\S+.ed\\S+\", '', new_str)\n",
    "    new_str = re.sub(r\"[0-9]+\", '', new_str)\n",
    "    return new_str\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\b\\w{3,}\\b')\n",
    "stop_words = list(set(stopwords.words(\"english\")))\n",
    "stop_words += list(string.punctuation)\n",
    "stop_words += ['__', '___']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ad6646-bef2-4f19-8558-68f5945f5eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = fetch_20newsgroups(subset='train', return_X_y=True)\n",
    "X_test, y_test = fetch_20newsgroups(subset='test', return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d7866d-51ba-4fa0-b21f-991c7e44c37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = list(map(rmv_emails_websites, X_train))\n",
    "X_test  = list(map(rmv_emails_websites, X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3973f405-c916-446d-8ab3-17c68e4aba6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(lowercase=True, \n",
    "                        stop_words=stop_words, \n",
    "                        tokenizer=tokenizer.tokenize, \n",
    "                        max_df=0.2,\n",
    "                        min_df=0.02\n",
    "                       )\n",
    "tfidf_train_sparse = tfidf.fit_transform(X_train)\n",
    "tfidf_train_df = pd.DataFrame(tfidf_train_sparse.toarray(), \n",
    "                        columns=tfidf.get_feature_names_out())\n",
    "tfidf_train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d4017f-9c2c-4f77-b208-1ceb64fa1052",
   "metadata": {
    "tags": []
   },
   "source": [
    "Будем использовать SVD. Оставим 20 компонентов по числу тем. Сигма - это та самая матрица весов $\\Sigma$, а V_T - это матрица важности контекстов по документам:\n",
    "\n",
    "![Image](svd.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa0eb91-1560-431d-b634-f96271ea8fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "lsa_obj = TruncatedSVD(n_components=20, n_iter=100, random_state=42)\n",
    "tfidf_lsa_data = lsa_obj.fit_transform(tfidf_train_df)\n",
    "Sigma = lsa_obj.singular_values_\n",
    "V_T = lsa_obj.components_.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2c2e2d-a143-44df-bb1f-ed20adb88081",
   "metadata": {},
   "source": [
    "Визуализируем сигму: вроде действительно первая самая важная, а остальные равномерно убывают. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a2e7e0-631e-4323-a52f-410f117c57c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.barplot(x=list(range(len(Sigma))), y = Sigma)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38481b7e-9a82-4fc2-b6b8-77ae3267f708",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_lsa = LogisticRegression()\n",
    "logreg     = LogisticRegression()\n",
    "logreg_param_grid = [{'tol':[0.0001, 0.0005, 0.001]}]\n",
    "grid_lsa_log = GridSearchCV(estimator=logreg_lsa,\n",
    "                        param_grid=logreg_param_grid, \n",
    "                        scoring='accuracy', cv=5,\n",
    "                        n_jobs=-1)\n",
    "grid_log = GridSearchCV(estimator=logreg,\n",
    "                        param_grid=logreg_param_grid, \n",
    "                        scoring='accuracy', cv=5,\n",
    "                        n_jobs=-1)\n",
    "best_lsa_logreg = grid_lsa_log.fit(tfidf_lsa_data, y_train).best_estimator_\n",
    "best_reg_logreg = grid_log.fit(tfidf_train_df, y_train).best_estimator_\n",
    "print(\"Accuracy of Logistic Regression on LSA train data is :\", best_lsa_logreg.score(tfidf_lsa_data, y_train))\n",
    "print(\"Accuracy of Logistic Regression with standard train data is :\", best_reg_logreg.score(tfidf_train_df, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35cd4a2-023f-499f-a95c-0aa32884cc3b",
   "metadata": {},
   "source": [
    "### LSA в gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e700c2-37d5-4ad1-bf02-129cbfee9bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load data\n",
    "df = pd.read_csv('Musical_instruments_reviews.csv', usecols=['reviewerID', 'reviewText'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926db6a4-0e78-44fa-ab4e-6723371aa940",
   "metadata": {},
   "source": [
    "Обработаем данные: удалим стоп-слова, приведем к нижнему регистру, сделаем стемминг - для английского языка его будет достаточно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2600ae-495e-4395-8c0a-34f696a5c321",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords, strip_punctuation \\\n",
    "                                        , preprocess_string, strip_short, stem_text\n",
    "\n",
    "# preprocess given text\n",
    "def preprocess(text):\n",
    "    \n",
    "    # clean text based on given filters\n",
    "    CUSTOM_FILTERS = [lambda x: x.lower(), \n",
    "                                remove_stopwords, \n",
    "                                strip_punctuation, \n",
    "                                strip_short, \n",
    "                                stem_text]\n",
    "    text = preprocess_string(text, CUSTOM_FILTERS)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# apply function to all reviews \n",
    "df['Text (Clean)'] = df['reviewText'].astype(str).apply(lambda x: preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4b48cc-8e95-487e-a04b-8bf92a98c95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e3ad83-2b1f-459d-84f8-eda09cbeb4f6",
   "metadata": {},
   "source": [
    "Создадим словарь (мешок слов), на этом месте у нас возникнет та самая исходная матрица document-term для дальнейшего ее разложения. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bb1082-130e-493e-bfad-4cc6bc34d9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "# create a dictionary with the corpus\n",
    "corpus = df['Text (Clean)']\n",
    "dictionary = corpora.Dictionary(corpus)\n",
    "\n",
    "# convert corpus into a bag of words\n",
    "bow = [dictionary.doc2bow(text) for text in corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a0c7e0-56e4-4348-a4e8-d4df5abfb7a3",
   "metadata": {},
   "source": [
    "[LSI](https://radimrehurek.com/gensim/models/lsimodel.html) в gensim - это реализация SVD. Но прежде чем раскладывать, нам нужно определиться с количеством тем. В отличие от датасета с новостями, тут мы не знаем его заранее и поэтому можем воспользоваться Coherence Model, которая оценит, насколько будут близки друг другу слова в кластерах, переберет разные разбивки и выведет оценки. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ed4884-9a93-4794-9341-1aca6785de62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LsiModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "for i in range(2,11):\n",
    "    lsi = LsiModel(bow, num_topics=i, id2word=dictionary)\n",
    "    coherence_model = CoherenceModel(model=lsi, texts=df['Text (Clean)'], dictionary=dictionary, coherence='c_v')\n",
    "    coherence_score = coherence_model.get_coherence()\n",
    "    print('Coherence score with {} clusters: {}'.format(i, coherence_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb69b44a-9d4e-4ebd-b6d3-4a89c8b2bdf4",
   "metadata": {},
   "source": [
    "Видимо, лучше всего 2 темы. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092e84e3-29d4-4b2c-9f86-282b1fb5b930",
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi = LsiModel(bow, num_topics=2, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24b2655-a74a-4237-b90d-114d6dbd5b1f",
   "metadata": {},
   "source": [
    "Посмотрим по пять первых слов в получившихся темах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f34d2e3-ba2e-44e2-8482-b42884876e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic_num, words in lsi.print_topics(num_words=5):\n",
    "    print('Words in {}: {}.'.format(topic_num, words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db71a967-fe60-490e-8d40-2f5b7fed5112",
   "metadata": {},
   "source": [
    "Попробуем в тестовом режиме оценить тематику ревью."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9105d7-c13a-4eac-9777-b750049dae29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the scores given between the review and each topic\n",
    "corpus_lsi = lsi[bow]\n",
    "score1 = []\n",
    "score2 = []\n",
    "for doc in corpus_lsi:\n",
    "    score1.append(round(doc[0][1],2))\n",
    "    score2.append(round(doc[1][1],2))\n",
    "\n",
    "# create data frame that shows scores assigned for both topics for each review\n",
    "df_topic = pd.DataFrame()\n",
    "df_topic['Text'] = df['reviewText']\n",
    "df_topic['Topic 0 score'] = score1\n",
    "df_topic['Topic 1 score'] = score2\n",
    "df_topic['Topic']= df_topic[['Topic 0 score', 'Topic 1 score']].apply(lambda x: x.argmax(), axis=1)\n",
    "df_topic.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8b8173-d0eb-4b78-b642-7e591efb957e",
   "metadata": {},
   "source": [
    "Выведем сэмплы для обеих тем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7853b11-334c-4018-b5f8-51579bebc333",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic0 = df_topic[df_topic['Topic'] == 0]\n",
    "df_topic1 = df_topic[df_topic['Topic']==1]\n",
    "print('Sample text from topic 0:\\n {}'.format(df_topic0.sample(1, random_state=2)['Text'].values))\n",
    "print('\\nSample text from topic 1:\\n {}'.format(df_topic1.sample(1, random_state=2)['Text'].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7715775f-39b6-49b8-b5be-a838824d7ed3",
   "metadata": {},
   "source": [
    "### LDA в sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e5a9dd-fae5-4f8c-a036-dc0dbf99f17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = list(pd.read_csv('google.csv')['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415a07f1-41a7-4738-a0da-1f818361dceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ed9145-8e72-42f8-800f-833540b18681",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pymorphy2\n",
    "from razdel import tokenize\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f060474-81df-49a3-94ca-190d98ceac3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(corpus)):\n",
    "    corpus[i] = re.sub(r'https://t\\.co/\\w+\\b', '', corpus[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce895114-c1a2-414f-a4d5-72ad2906cdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(string):\n",
    "    tokenized = [t.text for t in tokenize(string) if re.fullmatch(r'(?i)[а-я]+(-[а-я]+)*', t.text)]\n",
    "    return [morph.parse(token)[0].normal_form for token in tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248d9bbc-1c15-42f8-a9db-7578e3d3389d",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(tokenizer=lemmatize, stop_words=stopwords.words('russian'), lowercase=True)\n",
    "x_counts = count_vect.fit_transform(corpus)\n",
    "x_counts.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc686b0-1b62-464b-8deb-4b345e283982",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(count_vect.get_feature_names_out())\n",
    "features[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691b1ce3-2764-4852-ac73-da7fbc890d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "x_tfidf = tfidf_transformer.fit_transform(x_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71628a16-2d2b-42ac-8c1b-0c1ee19841f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension = 20\n",
    "lda = LDA(n_components = dimension)\n",
    "lda_array = lda.fit_transform(x_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ec0e20-319e-4106-a8cd-bdb64ec61a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "components = [lda.components_[i] for i in range(len(lda.components_))]\n",
    "important_words = [sorted(features, key = lambda x: components[j][features.index(x)], reverse = True)[:10] for j in range(len(components))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b11291-3f0a-458f-8c8a-123e6bc50049",
   "metadata": {},
   "outputs": [],
   "source": [
    "important_words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlearn",
   "language": "python",
   "name": "mlearn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
