{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2498b833-4df0-4ac6-93ef-65f9126b2ac8",
   "metadata": {},
   "source": [
    "Давайте попробуем написать многослойный перцептрон с нуля, пользуясь знаниями, полученными на лекции: реализуем его в numpy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dadbbf8-1eb8-4b11-b243-067ad0ff8b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62026dbf-c94d-43bc-81e0-5222b15d8a17",
   "metadata": {},
   "source": [
    "Мы с вами знаем, что основная часть нейронной сети - это нейрон, или линейная регрессия, пропущенная через функцию активации. Давайте напишем класс для нашего будущего нейрона, а заодно и функцию активации, через которую будет проходить сигнал:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5c27fb-c100-4e29-a904-164181667622",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    # Наша функция активации: f(x) = 1 / (1 + e^(-x))\n",
    "    # your code here\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, weights, bias):\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "\n",
    "    def feedforward(self, inputs):\n",
    "        # Умножаем входы на веса, прибавляем порог, затем используем функцию активации\n",
    "\n",
    "weights = np.array([0, 1]) # w1 = 0, w2 = 1\n",
    "bias = 4                   # b = 4\n",
    "n = Neuron(weights, bias)\n",
    "\n",
    "x = np.array([2, 3])       # x1 = 2, x2 = 3\n",
    "print(n.feedforward(x))    # 0.9990889488055994"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04c5fcf-4702-4e8e-86cb-64c8af452498",
   "metadata": {},
   "source": [
    "Чтобы создать полноценную нейронную сеть, нужно объединить нейроны в слой. Давайте реализуем следующую архитектуру:\n",
    "\n",
    "<img src=\"https://media.proglib.io/posts/2020/10/02/de81e6549b3e3c3bc1e3fdc78fe59f9c.png\" />\n",
    "\n",
    "У этой сети два входа, скрытый слой с двумя нейронами ($h_1$ и $h_2$) и выходной слой с одним нейроном ($o_1$). Обратите внимание, что входы для $o_1$ – это выходы из $h_1$ и $h_2$. Именно это создает из нейронов сеть.\n",
    "\n",
    "Давайте реализуем прямую связь для нашей нейронной сети."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8605151e-fd71-4a07-8858-5a39fe564516",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurNeuralNetwork:\n",
    "    '''\n",
    "    Нейронная сеть с:\n",
    "    - 2 входами\n",
    "    - скрытым слоем с 2 нейронами (h1, h2)\n",
    "    - выходным слоем с 1 нейроном (o1)\n",
    "    Все нейроны имеют одинаковые веса и пороги:\n",
    "    - w = [0, 1]\n",
    "    - b = 0\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        weights = np.array([0, 1])\n",
    "        bias = 0\n",
    "\n",
    "        # Используем класс Neuron, написанный выше\n",
    "        self.h1 = Neuron(weights, bias)\n",
    "        self.h2 = Neuron(weights, bias)\n",
    "        self.o1 = Neuron(weights, bias)\n",
    "\n",
    "    def feedforward(self, x):\n",
    "        out_h1 = self.h1.feedforward(x)\n",
    "        out_h2 = self.h2.feedforward(x)\n",
    "\n",
    "        # Входы для o1 - это выходы h1 и h2\n",
    "        out_o1 = self.o1.feedforward(np.array([out_h1, out_h2]))\n",
    "\n",
    "        return out_o1\n",
    "\n",
    "network = OurNeuralNetwork()\n",
    "x = np.array([2, 3])\n",
    "print(network.feedforward(x)) # 0.7216325609518421"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ff2ffe-8262-471f-9e62-b89d175fc373",
   "metadata": {},
   "source": [
    "Допустим, у нас есть следующие измерения:\n",
    "\n",
    "| Имя | Вес (в фунтах) | Рост (в дюймах) | Пол |\n",
    "|---|---|---|---|\n",
    "| Алиса | 133 (54.4 кг) | 65 (165,1 см) | Ж |\n",
    "| Боб | 160 (65,44 кг) | 72 (183 см) | М |\n",
    "| Чарли | 152 (62.2 кг) | 70 (178 см) | М |\n",
    "| Диана | 120 (49 кг) | 60 (152 см) | Ж |\n",
    "\n",
    "\n",
    "Давайте обучим нашу нейронную сеть предсказывать пол человека по его росту и весу."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6b6364-3d1f-4069-a237-ac1c820dc1f4",
   "metadata": {},
   "source": [
    "Мы будем представлять мужской пол как 0, женский – как 1, а также сдвинем данные, чтобы их было проще использовать:\n",
    "\n",
    "| Имя | Вес - 135 | Рост - 66 | Пол |\n",
    "|---|---|---|---|\n",
    "| Алиса | -2 | -1 | 1 |\n",
    "| Боб | 25 | 6 | 0 |\n",
    "| Чарли | 17 | 4 | 0 |\n",
    "| Диана | -15 | -6 | 1 |\n",
    "\n",
    "Сделаем сдвиг, чтобы было проще считать (обычно сдвигают на среднее арифметическое, но здесь циферки просто подобраны, чтобы потом было легче). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671d3484-e7d7-46e2-a9cb-e62bc848bc60",
   "metadata": {},
   "source": [
    "Прежде чем обучать нашу нейронную сеть, нам нужно как-то измерить, насколько \"хорошо\" она работает, чтобы она смогла работать \"лучше\". Это измерение и есть потери (loss).\n",
    "\n",
    "Мы используем для расчета потерь среднюю квадратичную ошибку (mean squared error, MSE):\n",
    "\n",
    "$$MSE = \\frac{1} {n} \\sum_{i=1}^{n} (y_{true} - y_{pred})^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a4e33a-b164-494c-b768-f26ff8c499d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(y_true, y_pred):\n",
    "    # y_true и y_pred - массивы numpy одинаковой длины\n",
    "    # your code here\n",
    "\n",
    "y_true = np.array([1, 0, 0, 1])\n",
    "y_pred = np.array([0, 0, 0, 0])\n",
    "\n",
    "print(mse_loss(y_true, y_pred)) # 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712d259e-b312-4466-9c23-25b15bd402d9",
   "metadata": {},
   "source": [
    "Теперь у нас есть четкая цель: минимизировать потери нейронной сети. Мы знаем, что можем изменять веса и пороги нейронов, чтобы изменить ее предсказания, но как нам делать это таким образом, чтобы минимизировать потери?\n",
    "\n",
    "Будем рассматривать функцию потерь как функцию от весов и порогов. Давайте отметим все веса и пороги нашей нейронной сети:\n",
    "\n",
    "<img src=\"https://media.proglib.io/posts/2020/10/03/b3ae8de6555967d30fd4092654358e18.png\" />\n",
    "\n",
    "Теперь мы можем записать функцию потерь как функцию от нескольких переменных:\n",
    "\n",
    "$$L(w_1, w_2, w_3, w_4, w_5, w_6, b_1, b_2, b_3)$$\n",
    "\n",
    "Предположим, мы хотим отрегулировать $w_1$. Как изменится значение потери L при изменении $w_1$? На этот вопрос может ответить частная производная $\\frac {\\partial L} {\\partial w_1}$. Как мы ее рассчитаем?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e07c42-ba20-4d0f-b03a-53ee6b8a2116",
   "metadata": {},
   "source": [
    "Прежде всего, давайте перепишем эту частную производную через $\\frac {\\partial y_{pred}} {\\partial w_1}$, воспользовавшись цепным правилом. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5498ba-7d64-492c-abff-05f4ede54b27",
   "metadata": {},
   "source": [
    "$\\frac{\\partial L} {\\partial w_1} = \\frac{\\partial L} {\\partial y_{pred}} * \\frac{\\partial y_{pred}}{\\partial w_1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703e7291-4a68-4770-9d26-25f936344dcc",
   "metadata": {},
   "source": [
    "Мы можем рассчитать $\\frac{\\partial L}{\\partial y_{pred}}$, поскольку $y_{true} = 1$, а значит, $L = (1 - y_{pred})^2$:\n",
    "\n",
    "$\\frac{\\partial L} {\\partial y_{pred}} = \\frac{\\partial (1 - y_{pred})^2} {\\partial y_{pred}} = -2(1 - y_{pred})$\n",
    "\n",
    "Теперь давайте решим, что делать с $\\frac{\\partial y_{pred}}{\\partial w_1}$. Обозначая выходы нейронов, как прежде, $h_1$, $h_2$ и $o_1$, получаем:\n",
    "\n",
    "$y_{pred} = o_1 = f(w_5 h_1 + w_6 h_2 + b_3)$\n",
    "\n",
    "Вспомните, что f() – это наша функция активации, сигмоида. Поскольку $w_1$ влияет только на $h_1$ (но не на $h_2$), мы можем снова использовать цепное правило и записать:\n",
    "\n",
    "$\\frac{\\partial y_{pred}}{\\partial w_1} = \\frac{\\partial y_{pred}}{\\partial h_1} * \\frac{\\partial h_1}{\\partial w_1}$\n",
    "\n",
    "$\\frac{\\partial y_{pred}}{\\partial h_1} = w_5 * f'(w_5 h_1 + w_6 h_2 + b_3)$\n",
    " \n",
    "Мы можем сделать то же самое для $\\frac{\\partial h_1}{\\partial w_1}$, снова применяя цепное правило:\n",
    "\n",
    "$h_1 = f(w_1 x_1 + w_2 x_2 + b_1)$\n",
    "\n",
    "$\\frac{\\partial h_1}{\\partial w_1} = x_1 * f'(w_1 x_1 + w_2 x_2 + b_1)$\n",
    " \n",
    " \n",
    "В этой формуле $x_1$ – это вес, а $x_2$ – рост. Вот уже второй раз мы встречаем $f'(x)$ – производную сигмоидной функции! Давайте вычислим ее:\n",
    "\n",
    "$f(x) = \\frac{1}{1 + e^{-x}}$\n",
    "\n",
    "$f`(x) = \\frac{e^{-x}}{(1 + e^{-x})^2} = f(x) * (1 - f(x))$\n",
    " \n",
    " \n",
    "Мы используем эту красивую форму для $f'(x)$ позже. На этом мы закончили! Мы сумели разложить $\\frac {\\partial L} {\\partial w_1}$ на несколько частей, которые мы можем рассчитать:\n",
    "\n",
    "$\\frac{\\partial L} {\\partial w_1} = \\frac{\\partial L} {\\partial y_{pred}} * \\frac{\\partial y_{pred}}{\\partial h_1} * \\frac{\\partial h_1}{\\partial w_1}$\n",
    "\n",
    "Такой метод расчета частных производных \"от конца к началу\" называется методом обратного распространения (backpropagation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712b8a99-2188-4a68-923e-25c70d701a84",
   "metadata": {},
   "source": [
    "Будем считать, что наш набор данных пока состоит из одной Алисы (стохастический градиентный спуск!):\n",
    "\n",
    "| Имя | Вес - 141 | Рост - 67 | Пол |\n",
    "|---|---|---|---|\n",
    "| Алиса | -2 | -1 | 1 |\n",
    "\n",
    "Давайте инициализируем все веса как 1, а все пороги как 0. Если мы выполним прямой проход по нейронной сети, то получим:\n",
    "\n",
    "$h_1 = f(w_1 x_1 + w_2 x_2 + b_1) = f(-2 - 1 + 0) = 0.0474$\n",
    "$h_2 = f(w_3 x_1 + w_4 x_2 + b_2) = 0.0474$\n",
    "$o_1 = f(w_5 h_1 + w_6 h_2 + b_3) = f(0.0474 + 0.0474 + 0) = 0.524$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476d90d6-63ea-4fb2-9854-962304ec7d2e",
   "metadata": {},
   "source": [
    "Наша сеть выдает $y_{pred} = 0.524$, что находится примерно на полпути между мужским полом (0) и женским (1). Давайте рассчитаем $\\frac{\\partial L} {\\partial w_1}$:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3717d6-2dbc-4ad4-b417-b7a9f1bcf1bb",
   "metadata": {},
   "source": [
    "    # посчитайте это самостоятельно!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7f857a-be25-44d4-a210-1dc2d6482333",
   "metadata": {},
   "source": [
    "Должно получиться 0.0214.\n",
    "\n",
    "Результат говорит нам, что при увеличении $w_1$ функция ошибки чуть-чуть повышается. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2c2922-5713-4685-beae-556abd95bdf8",
   "metadata": {},
   "source": [
    "Давайте используем алгоритм оптимизации под названием стохастический градиентный спуск (stochastic gradient descent), который определит, как мы будем изменять наши веса и пороги для минимизации потерь. Фактически, он заключается в следующей формуле обновления:\n",
    "\n",
    "$w_1 \\leftarrow w_1 - \\eta \\frac{\\partial L} {\\partial w_1}$\n",
    "\n",
    "$\\eta$ - гиперпараметр, называемый скоростью обучения. \n",
    "\n",
    "Итого, все, что нам нужно теперь делать - вычитать по описанной выше формуле. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03d31b7-00ef-427b-8ee3-bfd233910595",
   "metadata": {},
   "source": [
    "Процесс обучения сети будет выглядеть примерно так:\n",
    "\n",
    "1. Выбираем одно наблюдение из набора данных. Именно то, что мы работаем только с одним наблюдением, делает наш градиентный спуск стохастическим.\n",
    "2. Считаем все частные производные функции потерь по всем весам и порогам ($\\frac{\\partial L} {\\partial w_1}$, $\\frac{\\partial L} {\\partial w_2}$ и т.д.)\n",
    "3. Используем формулу обновления, чтобы обновить значения каждого веса и порога.\n",
    "4. Снова переходим к шагу 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e066ffc-65c6-48f7-9c3e-c068a09f446a",
   "metadata": {},
   "source": [
    "Итак, соберем финальный код для всей нашей сети:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247ba4da-b0e1-4493-ae55-32467fd430ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    # Наша функция активации: f(x) = 1 / (1 + e^(-x))\n",
    "    # your code here\n",
    "\n",
    "def deriv_sigmoid(x):\n",
    "    # Производная сигмоиды: f'(x) = f(x) * (1 - f(x))\n",
    "    # your code here\n",
    "\n",
    "def mse_loss(y_true, y_pred):\n",
    "    # y_true и y_pred - массивы numpy одинаковой длины\n",
    "    # your code here\n",
    "\n",
    "class OurNeuralNetwork:\n",
    "    '''\n",
    "    Нейронная сеть с:\n",
    "    - 2 входами\n",
    "    - скрытым слоем с 2 нейронами (h1, h2)\n",
    "    - выходной слой с 1 нейроном (o1)\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        # Веса\n",
    "        self.w1 = np.random.normal()\n",
    "        self.w2 = np.random.normal()\n",
    "        self.w3 = np.random.normal()\n",
    "        self.w4 = np.random.normal()\n",
    "        self.w5 = np.random.normal()\n",
    "        self.w6 = np.random.normal()\n",
    "\n",
    "        # Пороги\n",
    "        self.b1 = np.random.normal()\n",
    "        self.b2 = np.random.normal()\n",
    "        self.b3 = np.random.normal()\n",
    "\n",
    "    def feedforward(self, x):\n",
    "        # x - массив numpy с двумя элементами\n",
    "        h1 = sigmoid(self.w1 * x[0] + self.w2 * x[1] + self.b1)\n",
    "        h2 = sigmoid(self.w3 * x[0] + self.w4 * x[1] + self.b2)\n",
    "        o1 = sigmoid(self.w5 * h1 + self.w6 * h2 + self.b3)\n",
    "        return o1\n",
    "\n",
    "    def train(self, data, all_y_trues):\n",
    "        '''\n",
    "        - data - массив numpy (n x 2) numpy, n = к-во наблюдений в наборе. \n",
    "        - all_y_trues - массив numpy с n элементами.\n",
    "          Элементы all_y_trues соответствуют наблюдениям в data.\n",
    "        '''\n",
    "        learn_rate = 0.1\n",
    "        epochs = 1000 # сколько раз пройти по всему набору данных \n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for x, y_true in zip(data, all_y_trues):\n",
    "                # --- Прямой проход (эти значения нам понадобятся позже)\n",
    "                sum_h1 = self.w1 * x[0] + self.w2 * x[1] + self.b1\n",
    "                h1 = sigmoid(sum_h1)\n",
    "\n",
    "                sum_h2 = self.w3 * x[0] + self.w4 * x[1] + self.b2\n",
    "                h2 = sigmoid(sum_h2)\n",
    "\n",
    "                sum_o1 = self.w5 * h1 + self.w6 * h2 + self.b3\n",
    "                o1 = sigmoid(sum_o1)\n",
    "                y_pred = o1\n",
    "\n",
    "                # --- Считаем частные производные.\n",
    "                # --- Имена: d_L_d_w1 = \"частная производная L по w1\"\n",
    "                d_L_d_ypred = -2 * (y_true - y_pred)\n",
    "\n",
    "                # Нейрон o1\n",
    "                d_ypred_d_w5 = h1 * deriv_sigmoid(sum_o1)\n",
    "                d_ypred_d_w6 = h2 * deriv_sigmoid(sum_o1)\n",
    "                d_ypred_d_b3 = deriv_sigmoid(sum_o1)\n",
    "\n",
    "                d_ypred_d_h1 = self.w5 * deriv_sigmoid(sum_o1)\n",
    "                d_ypred_d_h2 = self.w6 * deriv_sigmoid(sum_o1)\n",
    "\n",
    "                # Нейрон h1\n",
    "                d_h1_d_w1 = x[0] * deriv_sigmoid(sum_h1)\n",
    "                d_h1_d_w2 = x[1] * deriv_sigmoid(sum_h1)\n",
    "                d_h1_d_b1 = deriv_sigmoid(sum_h1)\n",
    "\n",
    "                # Нейрон h2\n",
    "                d_h2_d_w3 = x[0] * deriv_sigmoid(sum_h2)\n",
    "                d_h2_d_w4 = x[1] * deriv_sigmoid(sum_h2)\n",
    "                d_h2_d_b2 = deriv_sigmoid(sum_h2)\n",
    "\n",
    "                # --- Обновляем веса и пороги\n",
    "                # Нейрон h1\n",
    "                self.w1 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w1\n",
    "                self.w2 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w2\n",
    "                self.b1 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_b1\n",
    "\n",
    "                # Нейрон h2\n",
    "                self.w3 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w3\n",
    "                self.w4 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w4\n",
    "                self.b2 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_b2\n",
    "\n",
    "                # Нейрон o1\n",
    "                self.w5 -= learn_rate * d_L_d_ypred * d_ypred_d_w5\n",
    "                self.w6 -= learn_rate * d_L_d_ypred * d_ypred_d_w6\n",
    "                self.b3 -= learn_rate * d_L_d_ypred * d_ypred_d_b3\n",
    "\n",
    "              # --- Считаем полные потери в конце каждой эпохи\n",
    "            if epoch % 10 == 0:\n",
    "                y_preds = np.apply_along_axis(self.feedforward, 1, data)\n",
    "                loss = mse_loss(all_y_trues, y_preds)\n",
    "                print(\"Epoch %d loss: %.3f\" % (epoch, loss))\n",
    "\n",
    "# Определим набор данных\n",
    "data = np.array([\n",
    "  [-2, -1],  # Алиса\n",
    "  [25, 6],   # Боб\n",
    "  [17, 4],   # Чарли\n",
    "  [-15, -6], # Диана\n",
    "])\n",
    "all_y_trues = np.array([\n",
    "  1, # Алиса\n",
    "  0, # Боб\n",
    "  0, # Чарли\n",
    "  1, # Диана\n",
    "])\n",
    "\n",
    "# Обучаем нашу нейронную сеть!\n",
    "network = OurNeuralNetwork()\n",
    "network.train(data, all_y_trues)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c68799-1404-44cf-af68-ad4e6dbbbc55",
   "metadata": {},
   "source": [
    "Потестируем на ранее не виденных данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44993fb9-739c-4718-ba17-f828d564bdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "emily = np.array([-7, -3]) # 128 фунтов (52.35 кг), 63 дюйма (160 см)\n",
    "frank = np.array([20, 2])  # 155 фунтов (63.4 кг), 68 дюймов (173 см)\n",
    "print(f\"Эмили: {network.feedforward(emily):.3f}\")\n",
    "print(f\"Фрэнк: {network.feedforward(frank):.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
